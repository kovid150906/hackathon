# Configuration for Narrative Consistency Checker

# Primary LLM Provider
# Options: huggingface, deepseek, groq, ollama, anthropic, openai, google
llm_provider: "huggingface" # HuggingFace: FREE, many models, generous limits

# Provider Configurations
providers:
  huggingface:
    model: "meta-llama/Meta-Llama-3-8B-Instruct" # Free, good reasoning
    temperature: 0.1
    max_tokens: 300
    cost_per_million_tokens: 0 # FREE

  deepseek:
    model: "deepseek-chat" # Free, excellent reasoning model
    temperature: 0.1
    max_tokens: 300
    cost_per_million_tokens: 0 # FREE (generous limits)

  groq:
    model: "llama-3.1-8b-instant" # Smaller, faster, 25x fewer tokens
    temperature: 0.1
    max_tokens: 300 # Token-efficient; enough for decision + short rationale
    cost_per_million_tokens: 0 # FREE

  ollama:
    base_url: "http://localhost:11434"
    model: "phi3:mini" # Only needs ~2GB RAM, better quality than llama3.2:1b
    temperature: 0.1
    num_ctx: 2048 # Lower context to fit in ~3.6GiB RAM; 8192 can exceed 5GiB due to KV cache
    cost_per_million_tokens: 0 # FREE (local)
    max_tokens: 300 # Bound output for speed/consistency

  anthropic:
    model: "claude-3-5-sonnet-20241022"
    temperature: 0.1
    max_tokens: 8192
    cost_per_million_tokens: 3000 # $3 per million input tokens

  openai:
    model: "gpt-4-turbo-preview"
    temperature: 0.1
    max_tokens: 4096
    cost_per_million_tokens: 10000 # $10 per million input tokens

  google:
    model: "gemini-1.5-pro"
    temperature: 0.1
    max_output_tokens: 8192
    cost_per_million_tokens: 1250 # $1.25 per million input tokens

# Embeddings Configuration
embeddings:
  model: "BAAI/bge-large-en-v1.5" # or "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu" # or "cuda" if GPU available
  batch_size: 32 # Safer on ~3.6GiB RAM; large batches can cause WSL OOM-kill

# Reranker Configuration
reranker:
  enabled: true
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  top_k: 50 # Get top 50 before reranking
  final_k: 20 # Keep top 20 after reranking

# Pathway Configuration
pathway:
  vector_store:
    dimension: 1024 # BGE-large dimension
    collection_name: "narrative_chunks"
  chunking:
    strategy: "semantic" # semantic, fixed, or hybrid
    chunk_size: 3000 # Larger chunks = fewer total chunks
    chunk_overlap: 200
    min_chunk_size: 1200 # Reduce number of small chunks

# Self-Consistency Configuration
self_consistency:
  enabled: true
  num_chains: 2 # Reduced to 2 for better API quota management; early_stop helps
  voting_strategy: "weighted"
  confidence_threshold: 0.6
  early_stop_confidence: 0.85 # Stop extra chains when very confident

# Ensemble Configuration
ensemble:
  enabled: false # Disabled for speed
  models:
    - provider: "groq"
      model: "llama-3.1-8b-instant"
      weight: 1.0
    # Keep only Groq models here to ensure Groq-only operation
  voting_strategy: "weighted" # majority, weighted, or soft
  min_agreement: 0.6

# Multi-Agent Configuration
multi_agent:
  enabled: false # Disabled for speed
  agents:
    prosecutor:
      role: "Find evidence AGAINST consistency"
      temperature: 0.2
    defender:
      role: "Find evidence FOR consistency"
      temperature: 0.2
    investigator:
      role: "Neutral fact-finding"
      temperature: 0.0
    judge:
      role: "Final decision based on all evidence"
      temperature: 0.0
  deliberation_rounds: 1 # Reduced from 3 to save tokens

# Evidence Extraction
evidence:
  max_passages: 20
  llm_passages: 15 # LLM sees top 15 (less noise, fewer tokens)
  min_relevance_score: 0.5
  context_window: 500 # Characters before/after match

# Output Configuration
output:
  generate_rationale: true
  rationale_max_length: 200
  save_detailed_logs: true
  log_directory: "logs"

# Performance
performance:
  cache_embeddings: true
  parallel_processing: true
  max_workers: 4
  cache_dir: ".cache" # Persistent embedding cache
